{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zstandard\n",
    "import polars as pl\n",
    "import requests\n",
    "import json\n",
    "import tempfile\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_process_zst(url:str) -> pl.DataFrame:    \n",
    "    # Create a temporary directory\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        # Download\n",
    "        response = requests.get(url)\n",
    "        zst_path = os.path.join(temp_dir, \"data.zst\")\n",
    "        with open(zst_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        # Decompress\n",
    "        json_path = os.path.join(temp_dir, \"data.jsonl\")\n",
    "        with open(zst_path, 'rb') as compressed_file:\n",
    "            with open(json_path, 'wb') as decompressed_file:\n",
    "                dctx = zstandard.ZstdDecompressor()\n",
    "                dctx.copy_stream(compressed_file, decompressed_file)\n",
    "\n",
    "        # File is in JSONL format\n",
    "        df = pl.read_ndjson(\n",
    "            json_path,\n",
    "            infer_schema_length=10000,\n",
    "            ignore_errors=True\n",
    "        )\n",
    "        \n",
    "    return df    \n",
    "\n",
    "print(\"Downloading and processing data...\")\n",
    "df = download_process_zst(\"https://the-eye.eu/redarcs/files/truerateme_comments.zst\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define extraction strategy\n",
    "while many different ways to guess the rating are possible, we choose the mean of all floats ranging from 0 to 10 within the comment text\n",
    "\n",
    "Alternate methods:\n",
    "- first float from 0 to 10 within the comment text\n",
    "- using an LLM or other NLP technique to extract the rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_floats(cmt_text:str) -> list:\n",
    "    return [float(match[0]) for match in re.findall(r\"[+-]?(\\d+(\\.\\d+)?)\", cmt_text)]\n",
    "    \n",
    "def first_float_extraction(cmt_text:str) -> float:\n",
    "    return next(filter(lambda x: 0 <= x <= 10, extract_all_floats(cmt_text)), None)\n",
    "\n",
    "def mean_of_floats_extraction(cmt_text:str):\n",
    "    floats = extract_all_floats(cmt_text)\n",
    "    return sum(floats) / len(floats) if floats else None\n",
    "\n",
    "extraction_method = mean_of_floats_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, join\n",
    "we choose to join only top-level comments to their respective posts (do this by link id).\n",
    "this is because we want to \n",
    "\n",
    "## weighing techniques\n",
    "we could do a simple average of all comments. However, instead, we choose to weight the average by the total rating of all comments, ignoring those with negative ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_level_comments = df.filter(\n",
    "    pl.col(\"parent_id\") == pl.col(\"link_id\")\n",
    ")\n",
    "\n",
    "extract_id = lambda x: re.sub(r\"t\\d_\", \"\", x)\n",
    "\n",
    "top_level_comments = top_level_comments.with_columns(\n",
    "    pl.col(\"link_id\").map_elements(extract_id, pl.String).alias(\"thread\"),             # Extract post ID\n",
    "    pl.col(\"body\").map_elements(extraction_method, pl.Float32).alias(\"rating\")         # Extract rating\n",
    ")\n",
    "\n",
    "rated_comments = top_level_comments.filter(\n",
    "    pl.col(\"rating\").is_not_null() & \n",
    "    (pl.col(\"rating\") >= 0) & \n",
    "    (pl.col(\"rating\") <= 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join comments onto posts\n",
    "reddit_posts = pl.read_parquet(\"reddit_posts.parquet\")\n",
    "\n",
    "rated_posts = reddit_posts.join(\n",
    "    rated_comments,\n",
    "    left_on  = \"id\",\n",
    "    right_on = \"thread\",\n",
    "    how      = \"inner\"\n",
    ").with_columns(\n",
    "    zeroed_score = pl.when(pl.col(\"score\") < 0).then(0).otherwise(pl.col(\"score\"))\n",
    ").group_by('id').agg(\n",
    "    mean_rating     = pl.col(\"rating\").mean().alias(\"mean_rating\"),\n",
    "    median_rating   = pl.col(\"rating\").median().alias(\"median_rating\"),\n",
    "    rating_stdev    = pl.col(\"rating\").std().fill_null(0).alias(\"rating_stdev\"),\n",
    "    weighted_rating = (pl.col(\"rating\") * pl.col(\"zeroed_score\")).sum() / pl.col(\"zeroed_score\").sum(),\n",
    "    rating_count    = pl.col(\"rating\").count().alias(\"rating_count\"),\n",
    ")\n",
    "\n",
    "# Join ratings back onto posts\n",
    "reddit_posts = reddit_posts.join(\n",
    "    rated_posts,\n",
    "    left_on  = \"id\",\n",
    "    right_on = \"id\",\n",
    "    how      = \"left\"\n",
    ").filter(\n",
    "    (pl.col(\"rating_count\") > 0) &\n",
    "    (pl.col(\"local_thumbnail_path\") != \"\")\n",
    ")\n",
    "\n",
    "reddit_posts.write_parquet(\"reddit_posts_rated.parquet\")\n",
    "reddit_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, filter out all imgur image not found pics\n",
    "# TODO: this should be done before everything in the other notebook\n",
    "import os\n",
    "from PIL import Image\n",
    "def is_imgur_imagenotfound(path:str):\n",
    "    file_size = os.path.getsize(path)\n",
    "    \n",
    "    with Image.open(path) as img:\n",
    "        width, height = img.size\n",
    "    \n",
    "    return file_size == 503 and width == 161 and height == 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_imgur_imagenotfound(\"thumbnails/6uohb4.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out imgur image not found pics\n",
    "reddit_posts = reddit_posts.filter(\n",
    "    (pl.col(\"local_thumbnail_path\").map_elements(is_imgur_imagenotfound, pl.Boolean) == False)\n",
    ")\n",
    "\n",
    "reddit_posts.write_parquet(\"reddit_posts_rated.parquet\")\n",
    "reddit_posts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
